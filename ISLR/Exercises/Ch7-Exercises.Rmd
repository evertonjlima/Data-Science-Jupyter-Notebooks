---
  output: html_document
---

Notebook prepared by [Everton Lima](https://github.com/evertonjlima)
 
# Introduction to Statistical Learning Solutions (ISLR)
## Ch 7 Exercises
## Table of Contents

### Conceptual Exercises
- [1](#1)
- [2](#2)
- [3](#3)
- [4](#4)
- [5](#5)
- [6](#6)
- [7](#7)

### Applied Exercises
- [5](#5)
- [6](#6)
- [7](#7)
- [8](#8)
- [9](#9)
- [10](#10)
- [11](#11)

## Conceptual Exercises

### 1<a name="1"></a>

Skipped.

### 2<a name="2"></a>
#### 2a<a name="2a"></a>
The penalty term will be very large, and the RSS will be ignored. The function is 0.

#### 2b<a name="2b"></a>
In this case g(x) is a quadratic function in order to minimize its first derivative.

#### 2c<a name="2c"></a>
Cubic function.

#### 2d<a name="2d"></a>
Quadratic function.

#### 2e<a name="2e"></a>
There is no penalty term, so the function perfectly interpolates all observations.

### 3<a name="3"></a>

```{r}
X=seq(-2,2,0.1)

Y=1+X-2*(X-1)^2*I(X>=1)
plot(X,Y,type='l')
```

The function is linear up until $X>1$, afterwords it is a quadratic function with negative slope. 

### 4<a name="4"></a>

```{r}
Y=1+I(0<=X & X<=2)-(X-1)*I(1<=X & X<=2)+3*((X-3)*I(3<=X & X<=4)+I(4<X & X<=5))
plot(X,Y,type='l')
```

### 5<a name="5"></a>
#### 5a<a name="5a"></a>
\hat{g_2} will perform better in the training RSS since it is more flexible due to more degrees of freedom.

#### 5b<a name="5b"></a>
\hat{g_1} might perform better in the test RSS since it is less likely that it will overfit the data. 

#### 5c<a name="5c"></a>
Both equations are the same since there is no penalty, the model perfectly interpolates all the data pointed.
 
## Applied Exercises

### 6<a name="6"></a>
#### 6a<a name="6a"></a>

Using the ``cv.glm`` function of the boot package one can easily calculate the cross validation error. This is shown below.

```{r}
library(boot)
library(ISLR)

set.seed(532)
poly.mse=c()
for(degree in 1:7){
  poly.fit=glm(wage~poly(age,degree,raw=T),data=Wage)
  mse=cv.glm(poly.fit,data = Wage,K=10)$delta[1]
  poly.mse=c(poly.mse,mse)
}
```

Plotting the data results in the graph below; the optimal choice of polynomial is a polynomial of fourth degree. It is important to keep in mind that cross validation results will depend on how the K-folds are partitioned, which may skew the results obtained. 

```{r}
plot(poly.mse,xlab='Degree of Polynomial',ylab='Cross Validation Error',type='l')
x=which.min(poly.mse)
points(x,poly.mse[x],pch=20,cex=2,col='red')
```

From the curve we can see that there is not a significant difference in the error between polynomials of degree 4 and onward. Keeping this in mind, one should select a polynomial that does not overfit the data. 

The results obtained agree with ANOVA as there is no significant reduction is the cross validation error for adding terms with degree higher than 4.

#### 6b<a name="6b"></a>

To answer this question we repeat the same process as before, but substituting the polynomial fit with a step function. Note that when using the ``cut`` function we need to change change the data at every step, otherwise when doing cross validation the cutoff values for age will differ for every fold making it unable to make predictions.

```{r}

set.seed(42)
step.mse=c()
for(br in 2:10){
  Wage.model=model.frame(wage~cut(age,br),data=Wage)
  names(Wage.model)=c('wage','age')
  
  step.fit=glm(wage~age,data=Wage.model)
  mse=cv.glm(step.fit,data = Wage.model,K=10)$delta[1]
  step.mse=c(step.mse,mse)
}

plot(step.mse,xlab='Degree of Polynomial',ylab='Cross Validation Error',type='l')
x=which.min(step.mse)
points(x,step.mse[x],pch=20,cex=2,col='red')
```

From the graph above we can see that the optimal number of cutoff points is 7.

### 7<a name="7"></a>

Below an overview of the features in the ``Wage`` dataset is given.

```{r}
summary(Wage[,c('wage','maritl','jobclass','race','health')])
```

First, we plot the scatter plot of ``wage`` and each of the predictors.

```{r}
feat=c('maritl','jobclass','race','health')
par(mfrow=c(2,2))
for(p in feat)
  boxplot(wage~.,data=Wage[,c('wage',p)])
```

### 8<a name="8"></a>

This problem can be solved by predicting pairs of variables of the Auto dataset and using orthogonal polynomials, as generated by the ``poly`` function.

```{r}
glm.fit=glm(mpg~poly(displacement,5),data = Auto)
summary(glm.fit)
```

Below you can observe a quadratic fit of regressing ``mpg`` onto ``poly(displacement,2)``.

```{r}
newdat=c()

glm.fit=glm(mpg~poly(displacement,2),data = Auto)
newdat$pred = predict(glm.fit, newdata = data.frame(
  displacement=seq(min(Auto$displacement), max(Auto$displacement), length.out = 100)))

plot(Auto[,c('displacement','mpg')])
lines(seq(min(Auto$displacement), max(Auto$displacement), length.out = 100),
      newdat$pred,type='l',col='red')
```

### 9<a name="9"></a>
#### 9a<a name="9a"></a>

```{r}
library(MASS)

attach(Boston)
poly.fit=glm(nox~poly(dis,3),data=Boston)
summary(poly.fit)

plot(Boston[,c('dis','nox')])
pred=predict(poly.fit,
             data.frame(dis=seq(min(dis),max(dis),length.out = 100)))

lines(seq(min(dis),max(dis),length.out = 100),
      pred,col='red')
```

#### 9b<a name="9b"></a>

```{r}
x=seq(min(dis),max(dis),length.out = 100)
clrs=rainbow(10)
plot(Boston[,c('dis','nox')])
rss=c()

for(d in 1:10){
  poly.fit=glm(nox~poly(dis,d),data = Boston)
  pred=predict(poly.fit,data.frame(dis=x))
  lines(x,pred,col=clrs[d])
  
  rss=c(rss,sum(poly.fit$residuals^2))
}

legend(x='topright',legend = 1:10,col=clrs,lty = c(1,1),lwd = c(2,2))
plot(rss,xlab='Degree of Polynomials',ylab='RSE',type='l')
```

#### 9c<a name="9c"></a>

This can be done by the same approach as question [6a](#6a). The minimum achieved in shown by the red point in the curve below.

```{r}
library(boot)

set.seed(532)
poly.mse=c()
for(degree in 1:7){
  poly.fit=glm(nox~poly(dis,degree,raw=T),data=Boston)
  mse=cv.glm(poly.fit,data = Boston,K=10)$delta[1]
  poly.mse=c(poly.mse,mse)
}

plot(poly.mse,type='l',xlab='Polynomial Degree',ylab='Cross Validation MSE')
points(which.min(poly.mse),poly.mse[which.min(poly.mse)],col='red',pch=20,cex=2)
```

#### 9d<a name="9d"></a>

```{r}
library(splines)
library(MASS)

spline.fit=lm(nox~bs(dis,df =4),data=Boston)
x=seq(min(Boston[,'dis']),max(Boston[,'dis']),length.out = 100)
y=predict(spline.fit,data.frame(dis=x))

plot(Boston[,c('dis','nox')],ylim=c(0,1))
lines(x,y,col=clrs[4])
```

#### 9e<a name="9e"></a>

```{r}

plot(Boston[,c('dis','nox')],ylim=c(0,1))
clrs=rainbow(10)

x=seq(min(Boston[,'dis']),max(Boston[,'dis']),length.out = 100)

rss=c()
for(df in 3:10){
  spline.fit=lm(nox~bs(dis,df=df),data=Boston)
  y=predict(spline.fit,data.frame(dis=x))
  lines(x,y,col=clrs[df])
  
  rss=c(rss,sum(spline.fit$residuals^2))
}

legend(x='topright',legend=3:10,text.col=clrs[3:10],text.width=0.5,bty = 'n',horiz = T)
plot(3:10,rss,xlab='Df',ylab='Train RSS',type='l')
```

#### 9f<a name="9f"></a>

As before with the step model obtained from using the ``cut`` function, if we automatically generate the cutoffs we must do so in the entire dataset. If not, predictions cannot be made. 

Why do you think this is a bad approach? How can we know where to place the knots when we only partially observe the data?

```{r}
library(boot)

set.seed(42)
spline.mse=c()

for(df in 3:10){
  Boston.model=model.frame(nox~bs(dis,df=df),data=Boston)  # Note that because we are automatically setting the 
  names(Boston.model)=c('nox','bs.dis')                    # cutoffs we must do so in the entire dataset, otherwise
                                                           # predictions cannot be made.

  spline.fit=glm(nox~bs.dis,data=Boston.model)
  mse=cv.glm(spline.fit,data=Boston.model,K=10)$delta[1]
  spline.mse=c(spline.mse,mse)
}

plot(3:10,spline.mse,type='l',xlab='Df',ylab='Cross Val. MSE for Splines')

x=which.min(spline.mse)
points(x+2,spline.mse[x],col='red',pch=20,cex=2)
```

It is clear that fitting model with 6 degree of freedom performs well in this dataset.

### 10<a name="10"></a>
#### 10a<a name="10a"></a>

First, select observations of the College dataset by sampling.

```{r}
library(ISLR)

set.seed(25)
train = sample(1:nrow(College),500)
test = -train
```

Reform forward step subset selection of the predictors; This is done by making of the ``regsubsets`` function from the ``leaps`` library. Moreover, the subset of predictors that have the best training MSE score are selected. 

```{r}
library(leaps)
forward=regsubsets(Outstate~.,data=College,method = 'forward',nvmax = 17)

plot(1/nrow(College)*summary(forward)$rss,type='l',xlab='Number of Predictors',ylab='Train MSE Score',xaxt='n')
axis(side=1,at=seq(1,17,2),labels = seq(1,17,2))
```

From the plot we see that a good choice is the model with 7 predictors. The predictors in this model are shown below.

```{r}
which(summary(forward)$which[7,-1])
```

#### 10b<a name="10b"></a>

Here we fit a GAM by making use of smoothing splines for each of the predictors selected, except 'Private' since it is a qualitative predictor.

```{r}
library(gam)
gam.fit=gam(Outstate~Private+s(Room.Board)+s(Personal)+s(PhD)+s(perc.alumni)+s(Expend)+s(Grad.Rate),data=College[train,])

par(mfrow = c(2, 3))
plot(gam.fit,se=T,col='blue')
```

#### 10c<a name="10c"></a>

```{r}
gam.pred=predict(gam.fit,College[test,])
gam.mse=mean((College[test,'Outstate']-gam.pred)^2)
gam.mse
```

Evaluating the model on the test set performs better than in the training set, since the MSE obtained is lower on the test set. Furthermore, below we can see that about 78% of the variance encountered in the data is explained by this model.

```{r}
gam.tss = mean((College[test,'Outstate'] - mean(College[test,'Outstate']))^2)
test.rss = 1 - gam.mse/gam.tss
test.rss
```

#### 10d<a name="10d"></a>

```{r}
summary(gam.fit)
```

From the output of ANOVA we can not that there is a significant evidence that a non-linear relationship for ``Expend``,``Grad.Rate``,``PhD``,``Personal``, and ``Room.Board`` is present.

### 11<a name="11"></a>
#### 11a<a name="11a"></a>

```{r}

set.seed(40)

n=100
K=sample(1:100,1)
X1=rnorm(n,mean=sample(1:100,1),sd=sample(1:25,1))
X2=rnorm(n,mean=sample(1:100,1),sd=sample(1:25,1))

Y=K+3*X1+7*X2
plot(Y,col='darkgray')
```

#### 11b<a name="11b"></a>

I chose the mean of X1 for the initial value of B1.

```{r}
B1=mean(X1)
B1
```

#### 11c<a name="11c"></a>

```{r}
a=Y-B1*X1
B2=lm(a~X2)$coef[2]
B2
```

#### 11d<a name="11d"></a>

```{r}
a=Y-B2*X2
B1=lm(a~X1)$coef[2]
B1
```


#### 11c<a name="11c"></a>

```{r}

step_b0=c()
step_b1=c()
step_b2=c()

for(step in 1:1000){
    a=Y-B1*X1
    m=lm(a~X2)
    B2=m$coef[2]
    
    a=Y-B2*X2
    m=lm(a~X1)
    B1=m$coef[2]
    B0=m$coef[1]
  
    step_b1=c(step_b1,B1)
    step_b2=c(step_b2,B2)
    step_b0=c(step_b0,B0)
}

plot(step_b0,xlab='Step',ylab='Estimates',col='black',ylim=c(0,100),xlim=c(1,100),type='l')
lines(step_b1,xlab='Step',col='red')
lines(step_b2,xlab='Step',col='blue')
```

From the plot we can see that the algorithm converges very fast, finding the correct values within less than 10 steps.

#### 11f<a name="11f"></a>

The same coefficients are chosen by using multiple linear regression.

```{r}
data.frame(B0,B1,B2)
coef(lm(Y~X1+X2))
```

#### 11d<a name="11d"></a>

The algorithm converges in the first iteration.

```{r}
sum(step_b0!=B0)
sum(step_b1!=B1)
sum(step_b2!=B2)
```

### 12<a name="12"></a>

The intercept, coefficients, and observations are generated from the loop below.

```{r}
set.seed(42)

n=1000
p=100

Bs=c()
Xs=c()
for(i in 1:p){
  mu=sample(1:100,1)
  sd=sample(1:25,1)
  
  beta=rnorm(n = 1,mean = mu,sd = sd)
  x=rnorm(n = n,mean = mu,sd = sd)
  
  Bs=rbind(Bs,beta)
  Xs=cbind(Xs,x)
}

B0=sample(1:100,1)
Y=as.vector((Xs%*%Bs)+B0)
```

Now we can perform back fitting as before, with an added nested loop to iterate over the predictors for each step.

```{r}
maxstep=1000
diff=0.1

Bhs=c()
for(i in 1:100)                     # Guess starting values for each predictor.
  Bhs=rbind(Bhs,sample(1:100,1))

mse=mean((Bhs-Bs)^2)                   # Check how good approximation has been achieved
err=c(mse)

for(step in 1:maxstep){

  for(pred in 1:p){                 
    a=Y-Xs[,-pred] %*% Bhs[-pred,]     # Subtract all other predictors
    m=lm(a~Xs[,pred])                # Fit model for this predictor
    Bhs[pred]=m$coef[2]
  }
  
  mse=mean((Bhs-Bs)^2)                   # Check how good approximation has been achieved
  if( abs(mse-err[length(err)]) < diff){    # Stop when no significant change occurs
    err=c(err,mse)
    break
  } else {
    err=c(err,mse)
  }
}

plot(err,xlab='Iteration',ylab='MSE of Coefficients',type='l',
     main='Number of Iterations necessary \nfor convergency in Backfitting')
```

